{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework for portfolio optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **MADDPG Algorithm**: Five agents, each with an actor (policy) and critic (value estimator) neural network, learn to adjust their asset’s weight. They share the portfolio’s reward to encourage collaboration, using a replay buffer for stable training.\n",
    "- **Training**: The agents train over 500 episodes, each up to 252 steps (a trading year), updating policies with batched experiences. Exploration noise helps discover optimal allocations.\n",
    "- **Data**: Uses five years of daily stock returns, ensuring realistic market dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade yfinance plotly nbformat seaborn scipy torch gymnasium --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, asset_data, transaction_cost=0.001, max_steps=252):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.asset_data = asset_data  # DataFrame with asset prices\n",
    "        self.n_assets = asset_data.shape[1]\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # State: returns, volatility, weights\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.n_assets * 3,), dtype=np.float32\n",
    "        )\n",
    "        # Action: weight adjustments for each asset\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(self.n_assets,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action to adjust weights\n",
    "        weights_new = self.weights + action * 0.1  # Scale action for stability\n",
    "        weights_new = np.clip(weights_new, 0, 1)\n",
    "        weights_new /= np.sum(weights_new)  # Normalize to sum to 1\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        returns = self.asset_data.iloc[self.current_step].values\n",
    "        portfolio_return = np.sum(weights_new * returns)\n",
    "        \n",
    "        # Calculate portfolio volatility (simplified, based on recent returns)\n",
    "        window = min(self.current_step, 20)\n",
    "        recent_returns = self.asset_data.iloc[max(0, self.current_step - window):self.current_step].values\n",
    "        portfolio_vol = np.std(recent_returns @ weights_new) if window > 0 else 0.0\n",
    "        \n",
    "        # Reward: Approximate Sharpe ratio (return - risk penalty)\n",
    "        reward = portfolio_return - 0.5 * portfolio_vol ** 2\n",
    "        \n",
    "        # Transaction costs\n",
    "        cost = self.transaction_cost * np.sum(np.abs(weights_new - self.weights))\n",
    "        reward -= cost\n",
    "        \n",
    "        self.weights = weights_new\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= self.max_steps or self.current_step >= len(self.asset_data)\n",
    "        state = self._get_state()\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "    \n",
    "    def _get_state(self):\n",
    "        returns = self.asset_data.iloc[self.current_step].values\n",
    "        window = min(self.current_step, 20)\n",
    "        vols = self.asset_data.iloc[max(0, self.current_step - window):self.current_step].std().values\n",
    "        return np.concatenate([returns, vols, self.weights]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, total_action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + total_action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, total_action_dim, lr_actor=1e-4, lr_critic=1e-3, tau=0.01):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(state_dim, total_action_dim)\n",
    "        self.critic_target = Critic(state_dim, total_action_dim)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def select_action(self, state, noise_scale=0.1):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).detach().numpy()[0]\n",
    "        noise = noise_scale * np.random.randn(self.action_dim)\n",
    "        return np.clip(action + noise, -1.0, 1.0)\n",
    "    \n",
    "    def update(self, replay_buffer, batch_size, agents, gamma=0.99):\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(states[:, 0, :])  # [batch_size, state_dim]\n",
    "        action = torch.FloatTensor(actions.reshape(batch_size, -1))  # [batch_size, total_action_dim]\n",
    "        reward = torch.FloatTensor(rewards[:, 0])  # [batch_size]\n",
    "        next_state = torch.FloatTensor(next_states[:, 0, :])  # [batch_size, state_dim]\n",
    "        done = torch.FloatTensor(dones[:, 0])  # [batch_size]\n",
    "        \n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            next_actions = torch.cat([agent.actor_target(next_state) for agent in agents], dim=-1)  # [batch_size, total_action_dim]\n",
    "            q_next = self.critic_target(next_state, next_actions)\n",
    "            q_target = reward + gamma * q_next * (1 - done)\n",
    "        \n",
    "        q_value = self.critic(state, action)\n",
    "        critic_loss = nn.MSELoss()(q_value, q_target)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor update\n",
    "        pred_action = self.actor(state)\n",
    "        all_actions = []\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i == 0:  # Current agent's predicted action\n",
    "                all_actions.append(pred_action)\n",
    "            else:  # Other agents' actions from replay buffer\n",
    "                all_actions.append(torch.FloatTensor(actions[:, i, :]))\n",
    "        all_actions = torch.cat(all_actions, dim=-1)\n",
    "        actor_loss = -self.critic(state, all_actions).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, state_dim, action_dim, n_agents):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.state = np.zeros((max_size, n_agents, state_dim))\n",
    "        self.action = np.zeros((max_size, n_agents, action_dim))\n",
    "        self.reward = np.zeros((max_size, n_agents))\n",
    "        self.next_state = np.zeros((max_size, n_agents, state_dim))\n",
    "        self.done = np.zeros((max_size, n_agents))\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.done[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            self.state[idx],\n",
    "            self.action[idx],\n",
    "            self.reward[idx],\n",
    "            self.next_state[idx],\n",
    "            self.done[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d') # 5 years of data\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "max_episodes = 500\n",
    "max_steps = 252  # ~1 year of trading days\n",
    "batch_size = 64\n",
    "replay_buffer_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  5 of 5 completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "AAPL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AMZN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "GOOGL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSFT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TSLA",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bfa628ce-b182-450d-84ab-0e13afd2a3f3",
       "rows": [
        [
         "2025-01-21 00:00:00",
         "-0.03191582351493971",
         "0.021111818274402117",
         "0.010459242546479341",
         "-0.001235395649836435",
         "-0.00569752092797482"
        ],
        [
         "2020-09-10 00:00:00",
         "-0.032645810106685436",
         "-0.02860545100214451",
         "-0.013689099702834384",
         "-0.028018358662221576",
         "0.013814582587770685"
        ],
        [
         "2020-06-30 00:00:00",
         "0.00834783440671183",
         "0.029264486024190406",
         "0.014944488185736127",
         "0.02554902583133556",
         "0.0698073149894527"
        ],
        [
         "2020-07-17 00:00:00",
         "-0.0020205194670200877",
         "-0.01264375276193297",
         "0.00127386665641116",
         "-0.005099763733372842",
         "0.00013330494375263235"
        ],
        [
         "2024-07-19 00:00:00",
         "0.0005798131594776379",
         "-0.0033741230867346816",
         "-0.00016892206349672367",
         "-0.007402899416660835",
         "-0.040243947162209914"
        ],
        [
         "2023-07-12 00:00:00",
         "0.008985560924880565",
         "0.01568569880112225",
         "0.015280892905745302",
         "0.01422688069081035",
         "0.008154422401772665"
        ],
        [
         "2021-03-03 00:00:00",
         "-0.024456517504555464",
         "-0.02893170479822993",
         "-0.02570625645775515",
         "-0.026980697072249038",
         "-0.048423758283395624"
        ],
        [
         "2022-03-10 00:00:00",
         "-0.027186329642811202",
         "0.05412515560997311",
         "-0.007423863001206121",
         "-0.010086421713087823",
         "-0.024063755330148706"
        ],
        [
         "2023-12-15 00:00:00",
         "-0.002725662957057229",
         "0.01729753821347546",
         "0.005002279309344271",
         "0.01311729993101518",
         "0.009758999874367902"
        ],
        [
         "2023-07-19 00:00:00",
         "0.0070718684500425955",
         "0.019046892602732513",
         "-0.013978692769608614",
         "-0.012267422513468906",
         "-0.007090702250741598"
        ],
        [
         "2024-01-08 00:00:00",
         "0.02417480580488829",
         "0.026576703830634596",
         "0.022913231813028956",
         "0.018871575756595815",
         "0.012463646412957496"
        ],
        [
         "2025-03-26 00:00:00",
         "-0.009921793165153625",
         "-0.022264360904062586",
         "-0.03224671715951766",
         "-0.013133926493846482",
         "-0.05580626179068926"
        ],
        [
         "2024-10-21 00:00:00",
         "0.006297919625632309",
         "0.0004233124965837831",
         "0.003977510468327283",
         "0.0014827592599364614",
         "-0.008382378207102636"
        ],
        [
         "2022-02-02 00:00:00",
         "0.007044358098162062",
         "-0.0038427158906232384",
         "0.07523751867819373",
         "0.015222288266044881",
         "-0.02747919159891432"
        ],
        [
         "2022-07-26 00:00:00",
         "-0.008826131942715754",
         "-0.052253606265046715",
         "-0.023160611786516405",
         "-0.026774436008112024",
         "-0.0356637312112531"
        ],
        [
         "2023-11-30 00:00:00",
         "0.0030627849018582953",
         "-0.001571972217158657",
         "-0.018223746625489756",
         "0.0001584893533210785",
         "-0.016629792613843564"
        ],
        [
         "2022-07-19 00:00:00",
         "0.026721853572567733",
         "0.03911741266419111",
         "0.04384107131783366",
         "0.02076712542154091",
         "0.020716718219316865"
        ],
        [
         "2021-05-20 00:00:00",
         "0.02101217715697823",
         "0.004913709794237331",
         "0.015606517667207553",
         "0.013820127992355102",
         "0.04138712802336375"
        ],
        [
         "2022-08-17 00:00:00",
         "0.008784537298495154",
         "-0.01851079360669594",
         "-0.01766633521383365",
         "-0.0026362083672751835",
         "-0.008372333511224461"
        ],
        [
         "2025-01-10 00:00:00",
         "-0.024103798826267564",
         "-0.014360970473526025",
         "-0.009847917684861152",
         "-0.01321370717659709",
         "-0.0005064369418008585"
        ],
        [
         "2024-02-27 00:00:00",
         "0.00811436013677258",
         "-0.006810521779339163",
         "0.009522350101249355",
         "-0.00014713552041611955",
         "0.0016549741281637331"
        ],
        [
         "2021-09-22 00:00:00",
         "0.016872396666692602",
         "0.01089234819407503",
         "0.008994287910946142",
         "0.012822229547653663",
         "0.0169871810945057"
        ],
        [
         "2025-01-16 00:00:00",
         "-0.04040020223572849",
         "-0.012043887924316987",
         "-0.01350038302227774",
         "-0.0040581371761688745",
         "-0.03362756026209679"
        ],
        [
         "2022-10-05 00:00:00",
         "0.002053319637480344",
         "-0.001156159830559278",
         "-0.0020662551704461185",
         "0.0012857754624273632",
         "-0.034597517632881325"
        ],
        [
         "2024-06-26 00:00:00",
         "0.01999331269045146",
         "0.03901472799901873",
         "-0.0008150966382568514",
         "0.002683235292916475",
         "0.04814512260377568"
        ],
        [
         "2021-11-08 00:00:00",
         "-0.005552563905594443",
         "-0.008527948857282919",
         "0.0012026268683280605",
         "0.002767241671971732",
         "-0.04840070953386311"
        ],
        [
         "2024-05-24 00:00:00",
         "0.01658817878540697",
         "-0.0016570176564539851",
         "0.008297249341963076",
         "0.007400491468314785",
         "0.03165649721483632"
        ],
        [
         "2022-05-09 00:00:00",
         "-0.033189247265887345",
         "-0.05213354825727934",
         "-0.02795342109799903",
         "-0.036945408068030705",
         "-0.09072948807249681"
        ],
        [
         "2022-11-15 00:00:00",
         "0.011869364844227404",
         "0.004569038353104293",
         "0.02863126791106363",
         "0.0017386701757065026",
         "0.018172303095892017"
        ],
        [
         "2020-06-18 00:00:00",
         "0.0003982593942086776",
         "0.004922484646629588",
         "-0.012681347576839141",
         "0.010708229931777558",
         "0.012270733625858066"
        ],
        [
         "2022-01-25 00:00:00",
         "-0.011384684107005216",
         "-0.031533745543573244",
         "-0.02957862914521603",
         "-0.026588309590897974",
         "-0.012473124842489969"
        ],
        [
         "2023-09-25 00:00:00",
         "0.007380168228263706",
         "0.01665124873434287",
         "0.00660283090460112",
         "0.0016717942945150277",
         "0.008616467528091087"
        ],
        [
         "2022-10-21 00:00:00",
         "0.02705919256332101",
         "0.03531453097461368",
         "0.011603322464979415",
         "0.025280484014341464",
         "0.03454266549727758"
        ],
        [
         "2021-12-17 00:00:00",
         "-0.006501526892175535",
         "0.00678920370502567",
         "-0.018830618475890515",
         "-0.0033858205523797924",
         "0.006095449446181567"
        ],
        [
         "2022-11-18 00:00:00",
         "0.0037817260442889467",
         "-0.007485493894513584",
         "-0.009455144916591163",
         "-0.0019032497866326992",
         "-0.016269016527425406"
        ],
        [
         "2021-08-04 00:00:00",
         "-0.0027824352474344893",
         "-0.0034222398904129037",
         "-0.0037196114919209045",
         "-0.0021244813824099484",
         "0.001662548831768751"
        ],
        [
         "2021-01-27 00:00:00",
         "-0.00768384424199442",
         "-0.02812581228312927",
         "-0.04665222113159728",
         "0.0024534568606005003",
         "-0.02143608477064074"
        ],
        [
         "2023-08-21 00:00:00",
         "0.007736579113694075",
         "0.01095925117625085",
         "0.007139496418553826",
         "0.01706285297253607",
         "0.073274828918382"
        ],
        [
         "2020-12-15 00:00:00",
         "0.05009040923261954",
         "0.0025815971159888207",
         "0.005033436958886028",
         "-0.00032668618513242986",
         "-0.010284031966116491"
        ],
        [
         "2021-03-18 00:00:00",
         "-0.03390513169478526",
         "-0.03435878370750434",
         "-0.029237972224058884",
         "-0.02666221676125713",
         "-0.06932072763951169"
        ],
        [
         "2025-03-20 00:00:00",
         "-0.0052964103352275416",
         "-0.003017266841301769",
         "-0.0066507800472874745",
         "-0.0025269737708730755",
         "0.0016958954271570903"
        ],
        [
         "2022-10-11 00:00:00",
         "-0.010254983328618739",
         "-0.012844190270001499",
         "-0.006948712073587937",
         "-0.016750201046734636",
         "-0.02897383620084637"
        ],
        [
         "2020-12-23 00:00:00",
         "-0.006976312248059169",
         "-0.006627122071927394",
         "0.004656420847396481",
         "-0.013039359783107995",
         "0.008807772074100217"
        ],
        [
         "2020-10-12 00:00:00",
         "0.06352045389478866",
         "0.04754990740220588",
         "0.0358436603969392",
         "0.02590237614628399",
         "0.01912439516394837"
        ],
        [
         "2021-01-29 00:00:00",
         "-0.037420388334911125",
         "-0.009704654514695155",
         "-0.01394359372444387",
         "-0.029171790070357084",
         "-0.05015373491403896"
        ],
        [
         "2024-09-16 00:00:00",
         "-0.027775276778179814",
         "-0.008579580976924128",
         "0.0038104523590012374",
         "0.0017418087030378615",
         "-0.015241628421408637"
        ],
        [
         "2022-02-18 00:00:00",
         "-0.009355863356301342",
         "-0.013261957754408193",
         "-0.016116031633549466",
         "-0.009631079620356364",
         "-0.022103035296720752"
        ],
        [
         "2020-05-20 00:00:00",
         "0.019447997512879844",
         "0.019846275173532213",
         "0.025291014722777616",
         "0.013871199587952132",
         "0.00934395713390801"
        ],
        [
         "2021-03-01 00:00:00",
         "0.05385127596555961",
         "0.017203798394717307",
         "0.02361633028982446",
         "0.01962309238421911",
         "0.06355291148192554"
        ],
        [
         "2023-01-27 00:00:00",
         "0.013684351499824166",
         "0.03043737760442866",
         "0.018970512655838823",
         "0.0006451730951289036",
         "0.11000180416824756"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1255
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-21</th>\n",
       "      <td>-0.031916</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>-0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>-0.032646</td>\n",
       "      <td>-0.028605</td>\n",
       "      <td>-0.013689</td>\n",
       "      <td>-0.028018</td>\n",
       "      <td>0.013815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>0.014944</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.069807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-17</th>\n",
       "      <td>-0.002021</td>\n",
       "      <td>-0.012644</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-19</th>\n",
       "      <td>0.000580</td>\n",
       "      <td>-0.003374</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.007403</td>\n",
       "      <td>-0.040244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-11</th>\n",
       "      <td>0.072649</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>-0.018010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-23</th>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.045856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-14</th>\n",
       "      <td>0.016480</td>\n",
       "      <td>-0.006779</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.006244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>0.016913</td>\n",
       "      <td>-0.002920</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>-0.033201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-08</th>\n",
       "      <td>0.018404</td>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.015239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          AAPL      AMZN     GOOGL      MSFT      TSLA\n",
       "Date                                                        \n",
       "2025-01-21 -0.031916  0.021112  0.010459 -0.001235 -0.005698\n",
       "2020-09-10 -0.032646 -0.028605 -0.013689 -0.028018  0.013815\n",
       "2020-06-30  0.008348  0.029264  0.014944  0.025549  0.069807\n",
       "2020-07-17 -0.002021 -0.012644  0.001274 -0.005100  0.000133\n",
       "2024-07-19  0.000580 -0.003374 -0.000169 -0.007403 -0.040244\n",
       "...              ...       ...       ...       ...       ...\n",
       "2024-06-11  0.072649  0.000909  0.009200  0.011242 -0.018010\n",
       "2024-08-23  0.010288  0.005167  0.011111  0.002984  0.045856\n",
       "2024-10-14  0.016480 -0.006779  0.010537  0.006774  0.006244\n",
       "2023-09-18  0.016913 -0.002920  0.005895 -0.003513 -0.033201\n",
       "2024-10-08  0.018404  0.010619  0.008590  0.012624  0.015239\n",
       "\n",
       "[1255 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_data(tickers, start_date, end_date):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False,)['Adj Close']\n",
    "    returns = data.pct_change().dropna()\n",
    "    return returns\n",
    "\n",
    "asset_data = fetch_data(tickers, start_date, end_date)\n",
    "asset_data.sample(frac=1, random_state=42)  # Shuffle data for randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(asset_data)\n",
    "n_agents = len(tickers)\n",
    "state_dim = env.observation_space.shape[0]  # 15\n",
    "action_dim = 1  # Each agent controls one asset\n",
    "total_action_dim = n_agents * action_dim  # 5\n",
    "\n",
    "# Initialize agents\n",
    "agents = [\n",
    "    MADDPGAgent(state_dim, action_dim, total_action_dim)\n",
    "    for _ in range(n_agents)\n",
    "]\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, state_dim, action_dim, n_agents)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzitoh/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Reward: nan\n",
      "Episode 2/500, Reward: nan\n",
      "Episode 3/500, Reward: nan\n",
      "Episode 4/500, Reward: nan\n",
      "Episode 5/500, Reward: nan\n",
      "Episode 6/500, Reward: nan\n",
      "Episode 7/500, Reward: nan\n",
      "Episode 8/500, Reward: nan\n",
      "Episode 9/500, Reward: nan\n",
      "Episode 10/500, Reward: nan\n",
      "Episode 11/500, Reward: nan\n",
      "Episode 12/500, Reward: nan\n",
      "Episode 13/500, Reward: nan\n",
      "Episode 14/500, Reward: nan\n",
      "Episode 15/500, Reward: nan\n",
      "Episode 16/500, Reward: nan\n",
      "Episode 17/500, Reward: nan\n",
      "Episode 18/500, Reward: nan\n",
      "Episode 19/500, Reward: nan\n",
      "Episode 20/500, Reward: nan\n",
      "Episode 21/500, Reward: nan\n",
      "Episode 22/500, Reward: nan\n",
      "Episode 23/500, Reward: nan\n",
      "Episode 24/500, Reward: nan\n",
      "Episode 25/500, Reward: nan\n",
      "Episode 26/500, Reward: nan\n",
      "Episode 27/500, Reward: nan\n",
      "Episode 28/500, Reward: nan\n",
      "Episode 29/500, Reward: nan\n",
      "Episode 30/500, Reward: nan\n",
      "Episode 31/500, Reward: nan\n",
      "Episode 32/500, Reward: nan\n",
      "Episode 33/500, Reward: nan\n",
      "Episode 34/500, Reward: nan\n",
      "Episode 35/500, Reward: nan\n",
      "Episode 36/500, Reward: nan\n",
      "Episode 37/500, Reward: nan\n",
      "Episode 38/500, Reward: nan\n",
      "Episode 39/500, Reward: nan\n",
      "Episode 40/500, Reward: nan\n",
      "Episode 41/500, Reward: nan\n",
      "Episode 42/500, Reward: nan\n",
      "Episode 43/500, Reward: nan\n",
      "Episode 44/500, Reward: nan\n",
      "Episode 45/500, Reward: nan\n",
      "Episode 46/500, Reward: nan\n",
      "Episode 47/500, Reward: nan\n",
      "Episode 48/500, Reward: nan\n",
      "Episode 49/500, Reward: nan\n",
      "Episode 50/500, Reward: nan\n",
      "Episode 51/500, Reward: nan\n",
      "Episode 52/500, Reward: nan\n",
      "Episode 53/500, Reward: nan\n",
      "Episode 54/500, Reward: nan\n",
      "Episode 55/500, Reward: nan\n",
      "Episode 56/500, Reward: nan\n",
      "Episode 57/500, Reward: nan\n",
      "Episode 58/500, Reward: nan\n",
      "Episode 59/500, Reward: nan\n",
      "Episode 60/500, Reward: nan\n",
      "Episode 61/500, Reward: nan\n",
      "Episode 62/500, Reward: nan\n",
      "Episode 63/500, Reward: nan\n",
      "Episode 64/500, Reward: nan\n",
      "Episode 65/500, Reward: nan\n",
      "Episode 66/500, Reward: nan\n",
      "Episode 67/500, Reward: nan\n",
      "Episode 68/500, Reward: nan\n",
      "Episode 69/500, Reward: nan\n",
      "Episode 70/500, Reward: nan\n",
      "Episode 71/500, Reward: nan\n",
      "Episode 72/500, Reward: nan\n",
      "Episode 73/500, Reward: nan\n",
      "Episode 74/500, Reward: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     actions.append(action)\n\u001b[32m     12\u001b[39m actions = np.concatenate(actions)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m next_state, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Store transition\u001b[39;00m\n\u001b[32m     17\u001b[39m replay_buffer.add(\n\u001b[32m     18\u001b[39m     np.array([state] * n_agents),\n\u001b[32m     19\u001b[39m     np.array([actions.reshape(n_agents, action_dim)]),\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     np.array([done] * n_agents),\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mPortfolioEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.current_step += \u001b[32m1\u001b[39m\n\u001b[32m     51\u001b[39m done = \u001b[38;5;28mself\u001b[39m.current_step >= \u001b[38;5;28mself\u001b[39m.max_steps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_step >= \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.asset_data)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, reward, done, {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mPortfolioEnv._get_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m returns = \u001b[38;5;28mself\u001b[39m.asset_data.iloc[\u001b[38;5;28mself\u001b[39m.current_step].values\n\u001b[32m     58\u001b[39m window = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.current_step, \u001b[32m20\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m vols = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masset_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.values\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.concatenate([returns, vols, \u001b[38;5;28mself\u001b[39m.weights]).astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/frame.py:11748\u001b[39m, in \u001b[36mDataFrame.std\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11739\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11740\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstd\u001b[39m(\n\u001b[32m  11741\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11746\u001b[39m     **kwargs,\n\u001b[32m  11747\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11748\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11749\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11750\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/generic.py:12358\u001b[39m, in \u001b[36mNDFrame.std\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstd\u001b[39m(\n\u001b[32m  12351\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12352\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12356\u001b[39m     **kwargs,\n\u001b[32m  12357\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function_ddof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12359\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12360\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/generic.py:12322\u001b[39m, in \u001b[36mNDFrame._stat_function_ddof\u001b[39m\u001b[34m(self, name, func, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12319\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m  12320\u001b[39m     axis = \u001b[32m0\u001b[39m\n\u001b[32m> \u001b[39m\u001b[32m12322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddof\u001b[49m\n\u001b[32m  12324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/frame.py:11563\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11560\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11561\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m  11562\u001b[39m res = df._mgr.reduce(blk_func)\n\u001b[32m> \u001b[39m\u001b[32m11563\u001b[39m out = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m  11564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  11565\u001b[39m     out = out.astype(out_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/indexing.py:1754\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_integer(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/frame.py:4002\u001b[39m, in \u001b[36mDataFrame._ixs\u001b[39m\u001b[34m(self, i, axis)\u001b[39m\n\u001b[32m   4000\u001b[39m result = \u001b[38;5;28mself\u001b[39m._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m   4001\u001b[39m result._name = \u001b[38;5;28mself\u001b[39m.index[i]\n\u001b[32m-> \u001b[39m\u001b[32m4002\u001b[39m result = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__finalize__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4003\u001b[39m result._set_is_copy(\u001b[38;5;28mself\u001b[39m, copy=copy)\n\u001b[32m   4004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/generic.py:6262\u001b[39m, in \u001b[36mNDFrame.__finalize__\u001b[39m\u001b[34m(self, other, method, **kwargs)\u001b[39m\n\u001b[32m   6255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m other.attrs:\n\u001b[32m   6256\u001b[39m     \u001b[38;5;66;03m# We want attrs propagation to have minimal performance\u001b[39;00m\n\u001b[32m   6257\u001b[39m     \u001b[38;5;66;03m# impact if attrs are not used; i.e. attrs is an empty dict.\u001b[39;00m\n\u001b[32m   6258\u001b[39m     \u001b[38;5;66;03m# One could make the deepcopy unconditionally, but a deepcopy\u001b[39;00m\n\u001b[32m   6259\u001b[39m     \u001b[38;5;66;03m# of an empty dict is 50x more expensive than the empty check.\u001b[39;00m\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28mself\u001b[39m.attrs = deepcopy(other.attrs)\n\u001b[32m-> \u001b[39m\u001b[32m6262\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m = other.flags.allows_duplicate_labels\n\u001b[32m   6263\u001b[39m \u001b[38;5;66;03m# For subclasses using _metadata.\u001b[39;00m\n\u001b[32m   6264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._metadata) & \u001b[38;5;28mset\u001b[39m(other._metadata):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/reinforcement_learning/lib/python3.11/site-packages/pandas/core/flags.py:90\u001b[39m, in \u001b[36mFlags.allows_duplicate_labels\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@allows_duplicate_labels\u001b[39m.setter\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mallows_duplicate_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mbool\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m     value = \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThis flag\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms object has been deleted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        actions = []\n",
    "        for agent in agents:\n",
    "            action = agent.select_action(\n",
    "                state, noise_scale=0.1 * (1 - episode / max_episodes)\n",
    "            )\n",
    "            actions.append(action)\n",
    "        actions = np.concatenate(actions)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "\n",
    "        # Store transition\n",
    "        replay_buffer.add(\n",
    "            np.array([state] * n_agents),\n",
    "            np.array([actions.reshape(n_agents, action_dim)]),\n",
    "            np.array([reward] * n_agents),\n",
    "            np.array([next_state] * n_agents),\n",
    "            np.array([done] * n_agents),\n",
    "        )\n",
    "\n",
    "        # Update agents\n",
    "        if replay_buffer.size > batch_size:\n",
    "            for agent in agents:\n",
    "                agent.update(replay_buffer, batch_size, agents)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{max_episodes}, Reward: {episode_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, agent in enumerate(agents):\n",
    "    torch.save(agent.actor.state_dict(), f\"actor_agent_{i}.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"critic_agent_{i}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
