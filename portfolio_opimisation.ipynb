{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c5a8a",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Portfolio Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f8e10",
   "metadata": {},
   "source": [
    "This experiment demonstrates the application of Deep Reinforcement Learning (DRL) algorithms (`A2C`, `PPO`, `SAC`) for portfolio optimization.  \n",
    "The workflow includes:  \n",
    "- Fetching and preprocessing Dow 30 market data using FinRL’s `YahooDownloader` and `FeatureEngineer`.  \n",
    "- Splitting the data into training (2010–2024) and trading (2024–2024) datasets.  \n",
    "- Defining a Gym environment (`StockPortfolioEnv`) for portfolio allocation.  \n",
    "- Training three DRL agents (`A2C`, `PPO`, `SAC`) for 50k timesteps each.  \n",
    "- Backtesting each agent’s daily returns and computing performance statistics.  \n",
    "- Constructing a minimum-variance portfolio with `PyPortfolioOpt` and simulating its evolution.  \n",
    "- Retrieving DJIA benchmark returns for comparison.  \n",
    "- Plotting cumulative returns of all strategies side by side.  \n",
    "- Outputting detailed performance metrics for each approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb03e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "# ! conda install -n portfolio_opt ipykernel --update-deps --force-reinstall\n",
    "# ! pip install pandas_market_calendars quantstats gymnasium -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas numpy matplotlib \\\n",
    "               stable-baselines3 \\\n",
    "               PyPortfolioOpt \\\n",
    "               pandas_market_calendars quantstats gymnasium \\\n",
    "               git+https://github.com/AI4Finance-Foundation/FinRL.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51776d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Stable Baselines3 imports\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "\n",
    "# FinRL imports\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, get_daily_return, get_baseline, convert_daily_return_to_pyfolio_ts\n",
    "\n",
    "# PyPortfolioOpt imports\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83bcfe",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d2a4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Downloading data from 2005-01-01 to 2025-04-23'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Tickers: ['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (148857, 8)\n"
     ]
    }
   ],
   "source": [
    "ticker_list = config_tickers.DOW_30_TICKER\n",
    "start_date = '2005-01-01'\n",
    "end_date = (datetime.now() - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "display(f\"Downloading data from {start_date} to {end_date}\".format(start_date, end_date))\n",
    "display(f\"Tickers: {ticker_list}\")\n",
    "\n",
    "df = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=ticker_list).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a0aad",
   "metadata": {},
   "source": [
    "## Preprocess data and append technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "fe = FeatureEngineer(use_technical_indicator=True, use_turbulence=False)\n",
    "df_tech = fe.preprocess_data(df)\n",
    "\n",
    "df_tech.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4d1f5",
   "metadata": {},
   "source": [
    "# Compute covariance matrices and lists for state representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing covariance matrices...\")\n",
    "\n",
    "df_sorted = df_tech.sort_values(['date','tic'], ignore_index=True)\n",
    "df_sorted.index = df_sorted.date.factorize()[0]\n",
    "cov_list, return_list = [], []\n",
    "lookback = 252\n",
    "unique_dates = df_sorted.date.unique()\n",
    "\n",
    "for i in range(lookback, len(unique_dates)):\n",
    "    window = df_sorted.loc[i - lookback : i, :]\n",
    "    price_mat = window.pivot_table(index='date', columns='tic', values='close')\n",
    "    ret_mat = price_mat.pct_change().dropna()\n",
    "    return_list.append(ret_mat)\n",
    "    cov_list.append(ret_mat.cov().values)\n",
    "    \n",
    "# Merge back covariances\n",
    "df_cov = pd.DataFrame({'date': unique_dates[lookback:], 'cov_list': cov_list, 'return_list': return_list})\n",
    "df_merged = pd.merge(df_tech, df_cov, on='date', how='left')\n",
    "\n",
    "# Drop initial rows without cov_list\n",
    "df_final = df_merged[df_merged['cov_list'].notna()].reset_index(drop=True)\n",
    "\n",
    "display(df_final.shape)\n",
    "display(df_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e6fe0",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_start, train_end = '2021-01-01', '2024-01-01'\n",
    "train_start, train_end = '2010-01-01', '2024-01-01'\n",
    "trade_start, trade_end = '2024-01-01', end_date\n",
    "\n",
    "train_data = data_split(df_final, train_start, train_end)\n",
    "trade_data = data_split(df_final, trade_start, trade_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd4570",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acbb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dim = len(train_data.tic.unique())\n",
    "state_space = stock_dim\n",
    "tech_indicators = config.INDICATORS\n",
    "env_kwargs = {\n",
    "    \"stock_dim\": stock_dim,\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1e6,\n",
    "    \"transaction_cost_pct\": 0.001,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"state_space\": state_space,\n",
    "    \"action_space\": stock_dim,\n",
    "    \"tech_indicator_list\": tech_indicators,\n",
    "}\n",
    "print(\n",
    "    f\"Stock Dim: {stock_dim}, State Space: {state_space}, Indicators: {tech_indicators}\"\n",
    ")\n",
    "\n",
    "# Create Gym environments\n",
    "e_train = StockPortfolioEnv(df=train_data, **env_kwargs)\n",
    "env_train, _ = e_train.get_sb_env()\n",
    "e_trade = StockPortfolioEnv(df=trade_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc6bdb",
   "metadata": {},
   "source": [
    "## Train DRL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "# algos = [\"a2c\", \"ppo\"]\n",
    "algos = [\"a2c\", \"ppo\", \"sac\"]\n",
    "trained_models = {}\n",
    "for algo in algos:\n",
    "    print(f\"Training {algo.upper()}...\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    model = agent.get_model(algo)\n",
    "    trained = agent.train_model(model=model, tb_log_name=algo, total_timesteps=50000)\n",
    "    trained_models[algo] = trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c247b",
   "metadata": {},
   "source": [
    "Save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"results/models\"\n",
    "\n",
    "# Create directory for models if it doesn't exist\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Save trained models\n",
    "for algo, model in trained_models.items():\n",
    "    model.save(f\"results/models/{algo}_trained_model\")\n",
    "    print(f\"Model {algo} saved to results/models/{algo}_trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a19091",
   "metadata": {},
   "source": [
    "Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trained_models' not in locals() or not trained_models:\n",
    "    # Check if model files exist before loading\n",
    "    model_paths = {\n",
    "        \"a2c\": \"results/models/a2c_trained_model.zip\",\n",
    "        \"ppo\": \"results/models/ppo_trained_model.zip\",\n",
    "        \"sac\": \"results/models/sac_trained_model.zip\",\n",
    "    }\n",
    "    \n",
    "    if all(os.path.exists(path) for path in model_paths.values()):\n",
    "        trained_models = {\n",
    "            \"a2c\": A2C.load(model_paths[\"a2c\"]),\n",
    "            \"ppo\": PPO.load(model_paths[\"ppo\"]),\n",
    "            \"sac\": SAC.load(model_paths[\"sac\"]),\n",
    "        }\n",
    "        print(\"Models loaded successfully.\")\n",
    "    else:\n",
    "        print(\"One or more model files are missing. Please ensure all models are saved correctly.\")\n",
    "else:\n",
    "    print(\"Trained models are already set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da5e6c",
   "metadata": {},
   "source": [
    "## Backtest DRL strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca399842",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for algo, model in trained_models.items():\n",
    "    print(f\"Backtesting {algo.upper()}...\")\n",
    "    df_ret, _ = DRLAgent.DRL_prediction(model = model, environment = e_trade)\n",
    "    # Reconstruct cumulative account value from daily returns\n",
    "    df_ret['account_value'] = (df_ret['daily_return'] + 1).cumprod() * env_kwargs['initial_amount']\n",
    "    stats = backtest_stats(df_ret, value_col_name='account_value')\n",
    "    results[algo] = {'df': df_ret, 'stats': stats}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa5638",
   "metadata": {},
   "source": [
    "## Calculate minimum-variance portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ab70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating minimum-variance portfolio...\")\n",
    "\n",
    "dates = trade_data.date.unique()\n",
    "min_var_vals = [env_kwargs['initial_amount']]\n",
    "for i in range(len(dates)-1):\n",
    "    df_curr = trade_data[trade_data.date == dates[i]].reset_index(drop=True)\n",
    "    df_next = trade_data[trade_data.date == dates[i+1]].reset_index(drop=True)\n",
    "    cov_mat = np.array(df_curr.cov_list.values[0])\n",
    "    ef = EfficientFrontier(None, cov_mat, weight_bounds=(0,1))\n",
    "    ef.min_volatility()\n",
    "    w = ef.clean_weights()\n",
    "    prices = df_curr.close.values\n",
    "    next_prices = df_next.close.values\n",
    "    shares = np.array(list(w.values())) * min_var_vals[-1] / prices\n",
    "    min_var_vals.append(np.dot(shares, next_prices))\n",
    "    \n",
    "min_var_df = pd.DataFrame({'date': dates, 'account_value': min_var_vals})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50364f",
   "metadata": {},
   "source": [
    "## Fetch DJIA benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a90a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching DJIA benchmark...\")\n",
    "baseline = get_baseline(ticker=\"^DJI\", start=trade_start, end=trade_end)\n",
    "baseline_ret = get_daily_return(baseline, \"close\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3175c4",
   "metadata": {},
   "source": [
    "## Plot cumulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for algo in algos:\n",
    "    df_ret = results[algo][\"df\"]\n",
    "    cump = (df_ret[\"daily_return\"] + 1).cumprod() - 1\n",
    "    plt.plot(df_ret[\"date\"], cump, label=algo.upper())\n",
    "\n",
    "# Min-var and DJIA\n",
    "c_min = (min_var_df[\"account_value\"].pct_change() + 1).cumprod() - 1\n",
    "plt.plot(min_var_df[\"date\"], c_min, label=\"MIN_VAR\")\n",
    "c_dji = (baseline_ret + 1).cumprod() - 1\n",
    "plt.plot(baseline[\"date\"], c_dji, label=\"DJIA\")\n",
    "plt.legend()\n",
    "plt.title(\"Cumulative Return Comparison\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.savefig(\"results/cumulative_return_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3456294",
   "metadata": {},
   "source": [
    "## Review performance stats for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_stats = pd.DataFrame({algo.upper(): results[algo]['stats'] for algo in algos})\n",
    "display(perf_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
