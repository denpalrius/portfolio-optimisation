{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c5a8a",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Portfolio Optimization - Constrained-DRL framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f8e10",
   "metadata": {},
   "source": [
    "This experiement demonstrates the application of deep reinforcement learning (DRL) techniques for portfolio optimization.\n",
    "\n",
    "- Policy network architecture: **MLP backbone**\n",
    "- Compares `A2C`, `PPO`, `SAC`, `DDPG`, `TD3` all with simple MLPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb03e",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas numpy matplotlib \\\n",
    "#                stable-baselines3 \\\n",
    "#                PyPortfolioOpt \\\n",
    "#                pandas_market_calendars quantstats gymnasium \\\n",
    "#                git+https://github.com/AI4Finance-Foundation/FinRL.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51776d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzitoh/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/pyfolio/pos.py:25: UserWarning: Module \"zipline.assets\" not found; multipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, SAC, DDPG, TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent, DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, get_daily_return, get_baseline, backtest_plot\n",
    "\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60da0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8f66f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e16a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mlp_constrained_drl\"\n",
    "results_dir = f\"results/models/{experiment_name}\"\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c1461",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefbe65",
   "metadata": {},
   "source": [
    "Define training and trading/test periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542703af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training period: ('2013-05-06', '2023-05-04')\n",
      "Testing period: ('2023-05-05', '2025-05-03')\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2015-01-01\"\n",
    "end_date = (datetime.now() - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")  # Yesterday\n",
    "\n",
    "trade_period = 2  # 2 years for testing\n",
    "train_period = 10  # 10 years for training\n",
    "\n",
    "train_end_date = (\n",
    "    datetime.strptime(end_date, \"%Y-%m-%d\") - timedelta(days=trade_period * 365)\n",
    ").strftime(\"%Y-%m-%d\")\n",
    "train_start_date = (\n",
    "    datetime.strptime(train_end_date, \"%Y-%m-%d\") - timedelta(days=train_period * 365)\n",
    ").strftime(\"%Y-%m-%d\")\n",
    "test_start_date = (\n",
    "    datetime.strptime(train_end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    ").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "train_dates = (train_start_date, train_end_date)\n",
    "test_dates = (test_start_date, end_date)\n",
    "\n",
    "print(f\"Training period: {train_dates}\")\n",
    "print(f\"Testing period: {test_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a425c",
   "metadata": {},
   "source": [
    "- Fetch historical stock data for a given list of tickers within a specified date range.\n",
    "- We use the DOW_30_TICKER stocks\n",
    "- The data includes `date`, `close`, `high`, `low`, `open`, `volume`, and `tic` (ticker symbol).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c577ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2015-01-01 → 2025-05-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (76911, 8)\n"
     ]
    }
   ],
   "source": [
    "def download_data(tickers, start_date, end_date):\n",
    "    print(f\"Downloading {start_date} → {end_date}\")\n",
    "    return YahooDownloader(\n",
    "        start_date=start_date, end_date=end_date, ticker_list=tickers\n",
    "    ).fetch_data()\n",
    "\n",
    "\n",
    "df = download_data(config_tickers.DOW_30_TICKER, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb738f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We apply feature engineering to the dataset of stock data:\n",
    "\n",
    "- Add technical indicators (e.g., moving averages, RSI).\n",
    "- Calculate turbulence indicators, which measure market volatility.\n",
    "\n",
    "This Enhance the dataset with features that are critical for modeling market dynamics and making informed trading decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47dcd423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    fe = FeatureEngineer(use_technical_indicator=True, use_turbulence=True)\n",
    "    return fe.preprocess_data(df)\n",
    "\n",
    "\n",
    "df_feat = preprocess_data(df)\n",
    "\n",
    "# TODO: Normalise the data??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdeb82",
   "metadata": {},
   "source": [
    "## Covariance & Returns for State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002710f6",
   "metadata": {},
   "source": [
    "- Calculate the rolling covariance matrices and daily returns for the given dataset of stock prices.\n",
    "- This prepares the state representation (the state of the portfolio) for the RL models in the RL environments for portfolio optimization.\n",
    "- The **rolling covariance matrices** (`cov_list`) capture the relationships between asset returns, while the daily returns (`return_list`) provide information about recent price movements.\n",
    "- These metrics are critical for modeling the dynamics of the financial market and making informed trading decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b6f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795ee68045284127896a5952a1744eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing covariance and returns:   0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_covariance_and_returns(df_feat, lookback=252):\n",
    "    df_sorted = df_feat.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "    df_sorted.index = df_sorted.date.factorize()[0]\n",
    "    cov_list, return_list = [], []\n",
    "\n",
    "    dates = df_sorted.date.unique()\n",
    "    for i in tqdm(range(lookback, len(dates)), desc=\"Computing covariance and returns\"):\n",
    "        win = df_sorted.loc[i - lookback : i]\n",
    "        pm = win.pivot_table(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        rm = pm.pct_change().dropna()\n",
    "        cov_list.append(rm.cov().values)\n",
    "        return_list.append(rm)\n",
    "    df_cov = pd.DataFrame(\n",
    "        {\"date\": dates[lookback:], \"cov_list\": cov_list, \"return_list\": return_list}\n",
    "    )\n",
    "\n",
    "    return pd.merge(df_feat, df_cov, on=\"date\", how=\"left\").dropna(subset=[\"cov_list\"])\n",
    "\n",
    "\n",
    "df_all = compute_covariance_and_returns(df_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7d720",
   "metadata": {},
   "source": [
    "## Train/Trade split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886a8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_all, train_dates, test_dates):\n",
    "    train = data_split(df_all, *train_dates)\n",
    "    test = data_split(df_all, *test_dates)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = split_data(df_all, train_dates, test_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83bcfe",
   "metadata": {},
   "source": [
    "## Environment setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c894307",
   "metadata": {},
   "source": [
    "**Constrained-DRL framework**\n",
    "\n",
    "\n",
    "The key innovation of our constrained‐DRL framework is that it **bakes real trading frictions and budget dynamics directly into the MDP**, rather than treating them as post-hoc adjustments. Concretely:\n",
    "\n",
    "1. **Transaction Costs & Slippage**\n",
    "\n",
    "   * **Unconstrained DRL** typically assumes zero commissions and perfect execution.\n",
    "   * **Our framework** charges the agent a proportional fee (α bps) and a slippage penalty (β bps) **every time it rebalances**. Those costs are subtracted from the simulated portfolio value *before* computing the reward.\n",
    "\n",
    "2. **Liquidity Limits**\n",
    "\n",
    "   * Rather than letting the agent instantly shift 100% of its capital between assets, we cap each trade to a fraction of the asset’s *average daily volume*. If the agent requests more than, say, 5% of ADV in a single day, the order is only partially filled—mirroring real market depth constraints.\n",
    "\n",
    "3. **Dynamic Budget Tracking & Early Exit**\n",
    "\n",
    "   * The agent’s *available capital* evolves with gains and losses. If its portfolio value ever falls below a preset threshold (e.g. 50% of the starting capital), the episode terminates with a large negative penalty—discouraging reckless strategies that risk ruin.\n",
    "\n",
    "4. **Reward Function Redefined**\n",
    "\n",
    "   * Instead of raw portfolio return, we use the **log net return**:\n",
    "\n",
    "     $$\n",
    "       R_t = \\log\\frac{V_{t+1} - \\text{costs}_t}{V_t}\\,,\n",
    "     $$\n",
    "\n",
    "     where $\\text{costs}_t$ includes both transaction fees and slippage. This forces the agent to learn the trade-off between chasing small gains and paying execution costs.\n",
    "\n",
    "5. **State & Action Adjustments**\n",
    "\n",
    "   * **State** now includes not only price histories and technical indicators, but also the *current portfolio weights* (the Portfolio Vector Memory) and, optionally, market liquidity metrics (e.g. daily volume).\n",
    "   * **Action** remains a target weight vector over the N assets (plus cash), enforced via a softmax so weights sum to 1. The environment’s `step()` method translates that weight shift into actual trades, applies the cost/slippage, enforces volume caps, updates the internal capital, and then returns the new state and log-return reward.\n",
    "\n",
    "6. **Implementation in Code**\n",
    "\n",
    "   * We extended the standard `PortfolioOptimizationEnv` (from FinRL) by overriding its `step()` function to:\n",
    "\n",
    "     1. Compute desired weight change $\\Delta w$.\n",
    "     2. For each asset, calculate trade volume = $\\Delta w · V_t$, cap it by `liquidity_limit · ADV`.\n",
    "     3. Deduct `α|Δw|V_t + β|Δw|V_t` from the portfolio value.\n",
    "     4. Update holdings and compute next-day portfolio value using actual market returns.\n",
    "     5. Compute reward as the log of net portfolio change.\n",
    "     6. Check for early termination if `V_{t+1} < bankruptcy_threshold`.\n",
    "\n",
    "7. **Integrating with DRL Agents**\n",
    "\n",
    "   * All of our agents (A2C, PPO, SAC, DDPG, TD3) interact with this modified environment exactly as before. The only difference is that the gym‐style interface now fully accounts for costs and liquidity when returning the `(obs, reward, done, info)` tuple.\n",
    "   * Architectures like **EIIE** and **EI³** leverage the extra state inputs (previous weights, ADV data) to learn cost-aware policies that naturally avoid expensive churn and only trade when the expected gain exceeds the execution expense.\n",
    "\n",
    "By implementing these changes at the environment level, we ensure that *every* DRL algorithm and network architecture is forced to internalize real-world trading constraints during training, resulting in strategies that are both profitable and feasible to deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0102f4",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91c0a6",
   "metadata": {},
   "source": [
    "**Exploring environment modifications for code**\n",
    "\n",
    "- To fact in slippage and liquidity, we extend `StockPortfolioEnv` into a `ConstrainedPortfolioEnv`. \n",
    "- This should allow for the integration of transaction costs, liquidity capping, and memory of portfolio vectors while keeping the original MLP architecture.\n",
    "\n",
    "\n",
    "We fold all of the “real-world” logic into a custom Gym env by subclassing FinRL’s `StockPortfolioEnv` (or `PortfolioOptimizationEnv`) and overriding its `step()` to:\n",
    "\n",
    "1. Compute desired weight changes → translate into trade volumes\n",
    "2. Cap each volume by `liquidity_limit × ADV`\n",
    "3. Charge both a proportional commission and a slippage fee on that volume\n",
    "4. Update cash and holdings, recompute portfolio value\n",
    "5. Return the log net‐return as reward (with costs already deducted)\n",
    "6. Early-stop if value < bankruptcy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16754322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedPortfolioEnv(StockPortfolioEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        slippage_pct: float = 0.0005,\n",
    "        liquidity_limit: float = 0.05,\n",
    "        bankruptcy_threshold: float = 0.5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # note: pass df as first positional arg, all others via kwargs\n",
    "        super().__init__(df, **kwargs)\n",
    "        self.slippage_pct = slippage_pct\n",
    "        self.liquidity_limit = liquidity_limit\n",
    "        self.bankruptcy_threshold = bankruptcy_threshold\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        # 1) record previous portfolio value\n",
    "        prev_val = self.asset_memory[-1]\n",
    "\n",
    "        # 2) get today’s prices & volumes\n",
    "        today = self.date_list[self.day]    # this is the current date string\n",
    "        df_today = self.df[self.df.date == today]\n",
    "        prices = df_today.close.values      # shape (stock_dim,)\n",
    "        vols   = df_today.volume.values     # shape (stock_dim,)\n",
    "\n",
    "        # 3) cap the raw action (shares to trade) by liquidity_limit * volume\n",
    "        capped_action = np.zeros_like(action)\n",
    "        for i in range(self.stock_dim):\n",
    "            max_shares = int(vols[i] * self.liquidity_limit)\n",
    "            capped_action[i] = np.clip(action[i], -max_shares, max_shares)\n",
    "\n",
    "        # 4) perform the trades & commission as usual\n",
    "        obs, _, done, info = super().step(capped_action)\n",
    "\n",
    "        # 5) deduct slippage cost = sum(|shares_traded| * price * slippage_pct)\n",
    "        slippage_cost = np.sum(np.abs(capped_action) * prices * self.slippage_pct)\n",
    "        self.cash -= slippage_cost\n",
    "\n",
    "        # 6) compute new portfolio value & log-return reward\n",
    "        new_val = self.asset_memory[-1]\n",
    "        reward = np.log(new_val / prev_val)\n",
    "\n",
    "        # 7) early termination if ruined\n",
    "        if new_val < self.initial_amount * self.bankruptcy_threshold:\n",
    "            done = True\n",
    "            reward -= 10.0   # large penalty\n",
    "\n",
    "        return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_environment(train, test, fe):\n",
    "    stock_dim = train.tic.nunique()\n",
    "    base_kwargs = dict(\n",
    "        stock_dim=stock_dim,\n",
    "        hmax=100,\n",
    "        initial_amount=1e6,\n",
    "        transaction_cost_pct=0.001,     # must match StockPortfolioEnv’s arg\n",
    "        reward_scaling=1e-4,\n",
    "        state_space=stock_dim,\n",
    "        action_space=stock_dim,\n",
    "        tech_indicator_list=fe.tech_indicator_list,\n",
    "    )\n",
    "\n",
    "    raw_train_env = ConstrainedPortfolioEnv(\n",
    "        train,\n",
    "        slippage_pct=0.0005,\n",
    "        liquidity_limit=0.05,\n",
    "        bankruptcy_threshold=0.5,\n",
    "        **base_kwargs,\n",
    "    )\n",
    "    raw_test_env = ConstrainedPortfolioEnv(\n",
    "        test,\n",
    "        slippage_pct=0.0005,\n",
    "        liquidity_limit=0.05,\n",
    "        bankruptcy_threshold=0.5,\n",
    "        **base_kwargs,\n",
    "    )\n",
    "\n",
    "    env_train_sb3, _ = raw_train_env.get_sb_env()\n",
    "    return env_train_sb3, raw_train_env, raw_test_env, base_kwargs\n",
    "\n",
    "env_train_sb3, raw_train_env, raw_test_env, env_kwargs = configure_environment(\n",
    "    train, test, FeatureEngineer()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17273f79",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- Create instances of the StockPortfolioEnv class for both training and testing datasets.\n",
    "- It also wrap the training environment for use with Stable-Baselines3 (SB3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6387a",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb2b50",
   "metadata": {},
   "source": [
    "- We define the configuration for various RL models to be trained in the portfolio optimization environment.\n",
    "- The training environment (`env_train_sb3`) is wrapped for use with Stable-Baselines3 (SB3).\n",
    "- The SB3 environment provides the `state` and `action space` dimensions needed for configuring the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d544aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_models():\n",
    "    model_configs = [\n",
    "        (A2C, \"A2C\", {}),\n",
    "        (PPO, \"PPO\", {}),\n",
    "        (SAC, \"SAC\", {}),\n",
    "        (DDPG, \"DDPG\", {}),\n",
    "        (TD3, \"TD3\", {}),\n",
    "    ]\n",
    "    return model_configs\n",
    "\n",
    "\n",
    "model_configs = prepare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1272b4",
   "metadata": {},
   "source": [
    "Train multiple reinforcement learning (RL) models using the specified training environment and configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd2a6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(env_train_sb3, model_configs, save_dir, timesteps_override=None):\n",
    "    # Recommended timesteps\n",
    "    timesteps_map = {\n",
    "        \"A2C\": 150_000,  # on-policy, fewer passes\n",
    "        \"PPO\": 250_000,  # on-policy, more stable\n",
    "        \"SAC\": 1_000_000,  # off-policy, reuse via replay\n",
    "        \"DDPG\": 1_000_000,  # off-policy\n",
    "        \"TD3\": 1_000_000,  # off-policy with twin critics\n",
    "    }\n",
    "\n",
    "    models, training_times = {}, {}\n",
    "\n",
    "    for cls, name, kwargs in model_configs:\n",
    "        n_steps = (\n",
    "            timesteps_override\n",
    "            if timesteps_override is not None\n",
    "            else timesteps_map[name]\n",
    "        )\n",
    "        print(f\"Training {name} for {n_steps} timesteps…\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        model = cls(\"MlpPolicy\", env_train_sb3, verbose=0, **kwargs)\n",
    "        model.learn(total_timesteps=n_steps)\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_times[name] = (end_time - start_time) / 60\n",
    "        models[name] = model\n",
    "        model.save(f\"{save_dir}/{name}_mlp_model\")\n",
    "\n",
    "        print(f\"{name} training completed in {training_times[name]:.2f} minutes.\")\n",
    "\n",
    "    return models, training_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cf62a",
   "metadata": {},
   "source": [
    "**Timesteps**\n",
    "- On-policy methods (A2C, PPO)\n",
    "    - 200 k–500 k timesteps (~80–200 passes):\n",
    "    - PPO often converges around 200 k–300 k if you use a clip range of 0.2 and 10–20 epochs per rollout; A2C can need slightly fewer.\n",
    "- Off-policy methods (SAC, DDPG, TD3)\n",
    "    - 500 k–1 M+ timesteps:\n",
    "    - These algorithms reuse experience via replay, so they typically benefit from 1 million+ steps to fully explore the state–action space.\n",
    "\n",
    "**Guideline**:\n",
    "- PPO/EIIE (on-policy): 250 k timesteps\n",
    "- SAC/EI³ (off-policy): 500 k timesteps, bump to 1 M if you still see improvement\n",
    "- A2C/MLP (on-policy): 150 k timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15af1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training A2C for 150000 timesteps…\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConstrainedPortfolioEnv' object has no attribute 'date_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m models, training_times = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_train_sb3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps_override\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(env_train_sb3, model_configs, save_dir, timesteps_override)\u001b[39m\n\u001b[32m     20\u001b[39m start_time = time.time()\n\u001b[32m     22\u001b[39m model = \u001b[38;5;28mcls\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env_train_sb3, verbose=\u001b[32m0\u001b[39m, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m end_time = time.time()\n\u001b[32m     26\u001b[39m training_times[name] = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[39m, in \u001b[36mA2C.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[32m    194\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    200\u001b[39m ) -> SelfA2C:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/portfolio_opt/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mConstrainedPortfolioEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     20\u001b[39m prev_val = \u001b[38;5;28mself\u001b[39m.asset_memory[-\u001b[32m1\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2) get today’s prices & volumes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m today = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdate_list\u001b[49m[\u001b[38;5;28mself\u001b[39m.day]    \u001b[38;5;66;03m# this is the current date string\u001b[39;00m\n\u001b[32m     24\u001b[39m df_today = \u001b[38;5;28mself\u001b[39m.df[\u001b[38;5;28mself\u001b[39m.df.date == today]\n\u001b[32m     25\u001b[39m prices = df_today.close.values      \u001b[38;5;66;03m# shape (stock_dim,)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'ConstrainedPortfolioEnv' object has no attribute 'date_list'"
     ]
    }
   ],
   "source": [
    "models, training_times = train_models(\n",
    "    env_train_sb3, model_configs, results_dir, timesteps_override=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times_df = pd.DataFrame(\n",
    "    list(training_times.items()), columns=[\"model\", \"training_duration (min)\"]\n",
    ")\n",
    "\n",
    "training_times_df.to_csv(f\"{results_dir}/training_times.csv\", index=False)\n",
    "\n",
    "print(\"Training summary:\")\n",
    "display(training_times_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b1712",
   "metadata": {},
   "source": [
    "## Model loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13c75e",
   "metadata": {},
   "source": [
    "Load the trained models from memory for analysis without the need for time consuming retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_configs, results_dir):\n",
    "    models = {}\n",
    "    for _, name, _ in model_configs:\n",
    "        model_path = f\"{results_dir}/{name}_mlp_model.zip\"\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Loading saved model for {name}...\")\n",
    "            models[name] = globals()[name].load(model_path)\n",
    "        else:\n",
    "            print(f\"No saved model found for {name}.\")\n",
    "    return models\n",
    "\n",
    "\n",
    "# models = load_models(model_configs, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ec26e",
   "metadata": {},
   "source": [
    "## Backtesting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5f8c6",
   "metadata": {},
   "source": [
    "- Evaluates the performance of the RL models/algorithms in a trading environment.\n",
    "- We do this by calculating the **cumulative portfolio value** and **performance metrics** for each RL model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93284a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_rl_strategies(models, raw_env, env_kwargs):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Backtesting {name}…\")\n",
    "        # Simulate trading using the model in the raw_env environment\n",
    "        df_ret, _ = DRLAgent.DRL_prediction(\n",
    "            model=model, environment=raw_env, deterministic=True\n",
    "        )\n",
    "        df_ret[\"account_value\"] = (df_ret.daily_return + 1).cumprod() * env_kwargs[\n",
    "            \"initial_amount\"\n",
    "        ]\n",
    "        stats = backtest_stats(df_ret, value_col_name=\"account_value\")\n",
    "        results[name] = {\"df\": df_ret, \"stats\": stats}\n",
    "    return results\n",
    "\n",
    "\n",
    "results = backtest_rl_strategies(models, raw_test_env, env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffa025",
   "metadata": {},
   "source": [
    "### Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2835322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backtest_results():\n",
    "    for name, res in results.items():\n",
    "        print(f\"Plotting {name}…\")\n",
    "        backtest_plot(\n",
    "            account_value=res[\"df\"],\n",
    "            baseline_start=test_start_date,\n",
    "            baseline_end=end_date,\n",
    "            baseline_ticker=\"SPY\",\n",
    "            value_col_name=\"account_value\",\n",
    "        )\n",
    "\n",
    "\n",
    "plot_backtest_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945e551",
   "metadata": {},
   "source": [
    "## Benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930eb311",
   "metadata": {},
   "source": [
    "These benchmarks will provide baseline performance metrics for comparison with the RL strategies.\n",
    "We evaluate the performance of **Mean-Variance Optimization (MVO)** and simple benchmarks (**Equal-Weighted Portfolio** and **SPY**) in terms of returns, volatility, and cumulative portfolio value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36346696",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Mean-Variance Optimization Benchmark\n",
    "\n",
    "- **Objective**: Calculate the benchmark portfolio using **Mean-Variance Optimization (MVO)**.\n",
    "- **Purpose**: This function benchmarks the performance of a portfolio optimized for minimum volatility using **Modern Portfolio Theory (MPT)**.\n",
    "- **Comparison**: It allows us to compare the MPT strategy with other RL strategies by analyzing metrics like returns, volatility, and cumulative performance.\n",
    "\n",
    "##### Workflow:\n",
    "\n",
    "1. **Covariance Matrix**:\n",
    "\n",
    "   - Extract the covariance matrix of asset returns for each trading day in the test period.\n",
    "   - Use this matrix to model the relationships between asset returns.\n",
    "\n",
    "2. **Optimization**:\n",
    "\n",
    "   - Apply **Efficient Frontier** to minimize portfolio volatility.\n",
    "   - Compute the optimal weights for each asset in the portfolio.\n",
    "\n",
    "3. **Portfolio Value Calculation**:\n",
    "\n",
    "   - Calculate the portfolio's account value over time using the optimized weights and asset prices.\n",
    "\n",
    "4. **Performance Metrics**:\n",
    "   - Evaluate the portfolio's performance using metrics such as annual return, cumulative return, and volatility.\n",
    "   - Add the results to the `results` dictionary under the `\"MPT\"` key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee740d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mpt_benchmark(test, env_kwargs):\n",
    "    dates_test = test.date.unique()\n",
    "    min_vals = [env_kwargs[\"initial_amount\"]]\n",
    "    for i in range(len(dates_test) - 1):\n",
    "        curr = test[test.date == dates_test[i]]\n",
    "        nxt = test[test.date == dates_test[i + 1]]\n",
    "        covm = np.array(curr.cov_list.values[0])\n",
    "        ef = EfficientFrontier(None, covm, weight_bounds=(0, 1))\n",
    "        ef.min_volatility()\n",
    "        w = ef.clean_weights()\n",
    "        prices = curr.close.values\n",
    "        nextp = nxt.close.values\n",
    "        shares = np.array(list(w.values())) * min_vals[-1] / prices\n",
    "        min_vals.append(np.dot(shares, nextp))\n",
    "    min_df = pd.DataFrame({\"date\": dates_test, \"account_value\": min_vals})\n",
    "    stats_mpt = backtest_stats(min_df, value_col_name=\"account_value\")\n",
    "    return {\"df\": min_df, \"stats\": stats_mpt}\n",
    "\n",
    "\n",
    "mpt_benchmark = compute_mpt_benchmark(test, env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c8b66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Equal-Weighted Portfolio Benchmark\n",
    "\n",
    "- Calculate the performance of an **equal-weighted portfolio** benchmark.\n",
    "- This benchmark assumes that all assets in the portfolio are equally weighted, and their daily returns are averaged to compute the portfolio's overall return.\n",
    "\n",
    "##### Workflow:\n",
    "\n",
    "1. **Daily Returns Calculation**:\n",
    "\n",
    "   - Group the test dataset by `date`.\n",
    "   - Compute the percentage change (`pct_change`) in the `close` prices for each group.\n",
    "   - Calculate the mean of the daily percentage changes to represent the portfolio's daily return.\n",
    "\n",
    "2. **Cumulative Portfolio Value**:\n",
    "\n",
    "   - Reset the index of the daily returns to create a DataFrame (`ew_df`).\n",
    "   - Compute the cumulative product of the daily returns (`cumprod`) to calculate the portfolio's cumulative value over time.\n",
    "   - Multiply the cumulative returns by the initial portfolio value (`initial_amount`) to get the portfolio's account value.\n",
    "\n",
    "3. **Performance Metrics**:\n",
    "   - Use the `backtest_stats` function to calculate performance metrics for the equal-weighted portfolio, such as annual return, cumulative return, and volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_equal_weighted_benchmark(df, initial_amount=100_000):\n",
    "    # Pivot to have one column per ticker\n",
    "    price_wide = df.pivot_table(\n",
    "        index=\"date\", columns=\"tic\", values=\"close\"\n",
    "    ).sort_index()\n",
    "\n",
    "    # Compute each ticker's daily return, then average equally\n",
    "    daily_rets = price_wide.pct_change().fillna(0).mean(axis=1)\n",
    "\n",
    "    # Build the equity curve\n",
    "    ew_df = pd.DataFrame({\"date\": daily_rets.index, \"daily_return\": daily_rets.values})\n",
    "    ew_df[\"account_value\"] = (ew_df[\"daily_return\"] + 1).cumprod() * initial_amount\n",
    "\n",
    "    # Compute performance statistics\n",
    "    stats_ew = backtest_stats(ew_df, value_col_name=\"account_value\")\n",
    "\n",
    "    return {\"df\": ew_df.reset_index(drop=True), \"stats\": stats_ew}\n",
    "\n",
    "\n",
    "ew_benchmark = compute_equal_weighted_benchmark(test, env_kwargs[\"initial_amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa738ed6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SPY Benchmark\n",
    "\n",
    "- **Objective**: Calculate the benchmark performance of the `SPY ETF`, which tracks the **S&P 500** index.\n",
    "- **Purpose**: This function provides a baseline for comparing the performance of reinforcement learning models and other portfolio strategies.\n",
    "\n",
    "##### Workflow:\n",
    "\n",
    "1. **Data Retrieval**:\n",
    "   - Use the `get_baseline` function to fetch the historical closing prices of the SPY ETF for the test period.\n",
    "2. **Daily Returns Calculation**:\n",
    "   - Compute the percentage change (`pct_change`) in the SPY closing prices to calculate daily returns.\n",
    "3. **Cumulative Portfolio Value**:\n",
    "   - Create a DataFrame (`spy_df`) with the daily returns and calculate the cumulative product (`cumprod`) of the daily returns to compute the portfolio's cumulative value over time.\n",
    "   - Multiply the cumulative returns by the initial portfolio value (`initial_amount`) to get the portfolio's account value.\n",
    "4. **Performance Metrics**:\n",
    "   - Use the `backtest_stats` function to calculate performance metrics for the SPY benchmark, such as annual return, cumulative return, and volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spy_benchmark(test, env_kwargs):\n",
    "    spy_close = get_baseline(\"SPY\", test_start_date, end_date)[\"close\"]\n",
    "    spy_ret = spy_close.pct_change().dropna()\n",
    "    spy_df = pd.DataFrame({\"date\": spy_ret.index, \"daily_return\": spy_ret.values})\n",
    "    spy_df[\"account_value\"] = (spy_df.daily_return + 1).cumprod() * env_kwargs[\n",
    "        \"initial_amount\"\n",
    "    ]\n",
    "    stats_spy = backtest_stats(spy_df, value_col_name=\"account_value\")\n",
    "    return {\"df\": spy_df, \"stats\": stats_spy}\n",
    "\n",
    "\n",
    "spy_benchmark = compute_spy_benchmark(test, env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a147661",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = {\n",
    "    \"MPT\": mpt_benchmark,\n",
    "    \"EW\": ew_benchmark,\n",
    "    \"SPY\": spy_benchmark,\n",
    "}\n",
    "\n",
    "results.update(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c912897",
   "metadata": {},
   "source": [
    "## Performance Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_stats = pd.DataFrame({key.upper(): res[\"stats\"] for key, res in results.items()})\n",
    "display(perf_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_metrics = [\n",
    "    \"Cumulative returns\",\n",
    "    \"Annual return\",\n",
    "    \"Annual volatility\",\n",
    "    \"Sharpe ratio\",\n",
    "    \"Max drawdown\",\n",
    "]\n",
    "\n",
    "# Filter the performance statistics for the selected metrics\n",
    "comparison_table = perf_stats.loc[comparison_metrics]\n",
    "\n",
    "\n",
    "# Plot the comparison metrics as a bar chart\n",
    "comparison_table.T.plot(kind=\"bar\", figsize=(16, 8))\n",
    "plt.title(\"Comparison of Key Metrics Across Models\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd5844",
   "metadata": {},
   "source": [
    "Visualize the cumulative returns of various strategies over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_returns(results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name, res in results.items():\n",
    "        # Ensure the date column is converted to datetime\n",
    "        res[\"df\"][\"date\"] = pd.to_datetime(res[\"df\"][\"date\"])\n",
    "        # Filter data to start from the trade start date\n",
    "        filtered_df = res[\"df\"][res[\"df\"][\"date\"] >= test_start_date]\n",
    "        cum = (\n",
    "            (filtered_df[\"daily_return\"] + 1).cumprod() - 1\n",
    "            if \"daily_return\" in filtered_df\n",
    "            else filtered_df[\"account_value\"] / filtered_df[\"account_value\"].iloc[0] - 1\n",
    "        )\n",
    "        plt.plot(filtered_df[\"date\"], cum, label=name)\n",
    "    plt.title(\"Cumulative Returns\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Cumulative Return\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cumulative_returns(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
